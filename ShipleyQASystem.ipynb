{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import sklearn as skl\n",
    "import nltk\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import csv\n",
    "import operator\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pandas import *\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET STOP WORDS\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "#companiesTraining = pd.read_csv('D:\\\\Homework3\\\\all\\companies.csv', delimiter=',', encoding='latin-1', header=None, names=[\"Companies Training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OPEN NEWS ARTICLES INTO PYTHON MEMORY\n",
    "\n",
    "######################################################################################################################\n",
    "################################### INSERT LOCATION OF 2013 AND 2014 FOLDERS HERE #####################################\n",
    "########################################################################################################################\n",
    "texts = []\n",
    "location = 'D:\\\\HW4\\\\2013'\n",
    "for filename in os.listdir(location):\n",
    "    fullfilename = location + '\\\\' + filename\n",
    "    with open(fullfilename, 'r', encoding='latin-1') as myfile:\n",
    "        text = myfile.read()\n",
    "        text = nltk.sent_tokenize(text)\n",
    "        for i in np.arange(len(text)):\n",
    "            texts.append(text[i])\n",
    "        myfile.close()\n",
    "\n",
    "location = 'D:\\\\HW4\\\\2014'\n",
    "for filename in os.listdir(location):\n",
    "    fullfilename = location + '\\\\' + filename\n",
    "    with open(fullfilename, 'r', encoding='latin-1') as myfile:\n",
    "        text = myfile.read()\n",
    "        text = nltk.sent_tokenize(text)\n",
    "        for i in np.arange(len(text)):\n",
    "            texts.append(text[i])\n",
    "        myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TOKENIZE ARTICLES INTO SENTENCES AND INDEX INTO ELASTICSEARCH\n",
    "\n",
    "client = Elasticsearch()\n",
    "actions = []\n",
    "\n",
    "for j in range(0,len(texts)):\n",
    "    currSentence = texts[j]\n",
    "    action = {\n",
    "        \"_index\": \"articles-index\",\n",
    "        \"_type\": \"articles\",\n",
    "        \"_id\": j,\n",
    "        \"_source\": {\n",
    "            \"any\": \"data\" + str(j),\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"body\": currSentence\n",
    "        }\n",
    "    }\n",
    "    actions.append(action)\n",
    "    \n",
    "if len(actions) > 0:\n",
    "    helpers.bulk(client, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXTRACT_ENTITIES ACCEPTS A SENTENCE AND OUTPUTS THE NAMED-ENTITIES IN A LIST\n",
    "\n",
    "def extract_entities(sent):\n",
    "    if type(sent) == str:\n",
    "        namedEntities = ne_chunk(pos_tag(word_tokenize(sent)))\n",
    "    else:\n",
    "        namedEntities = ne_chunk(pos_tag(sent))\n",
    "    continuous = []\n",
    "    current = []\n",
    "    \n",
    "    for i in namedEntities:\n",
    "        if type(i) == Tree:\n",
    "            current.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current:\n",
    "            named_entity = \" \".join(current)\n",
    "            if named_entity not in continuous:\n",
    "                continuous.append(named_entity)\n",
    "                current = []\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# QUESTION CLASSIFICATION\n",
    "# Function that accepts a sentence string as input and outputs question type\n",
    "\n",
    "# 1: Which companies went bankrupt in month X of year Y?\n",
    "# 2: What affects GDP?\n",
    "# 3: What percentage of drop or increase is associated with this property?\n",
    "# 4: Who is the CEO of company X?\n",
    "\n",
    "def classifyQuestion(candidate) :\n",
    "    '''\n",
    "    Returns 0, 1, 2, or 3 corresponding to question type 1, 2, 3, or 4\n",
    "    '''\n",
    "    question1 = \"which companies went bankrupt in month x of year y?\"\n",
    "    question2 = \"what affects gdp?\"\n",
    "    question3 = \"what percentage % of drop or increase change in gdp is associated with results from this property consumption consumer spending government investment imports exports foreign trade?\"\n",
    "    question4 = \"who is the ceo of company x?\"\n",
    "\n",
    "    questions = []\n",
    "    questions.append(question1)\n",
    "    questions.append(question2)\n",
    "    questions.append(question3)\n",
    "    questions.append(question4)\n",
    "    \n",
    "    candidate = candidate.lower()\n",
    "    candidateTokenized = nltk.word_tokenize(candidate)\n",
    "    questionsTokenized = []\n",
    "    cosineSimilarity = np.zeros(4)\n",
    "    candidateNorm = np.sqrt(len(candidateTokenized))\n",
    "\n",
    "    for i in np.arange(len(questions)):\n",
    "        questionsTokenized.append(nltk.word_tokenize(questions[i]))\n",
    "        questionNorm = np.sqrt(len(questionsTokenized[i]))\n",
    "        match = 0\n",
    "        for j in np.arange(len(candidateTokenized)):\n",
    "            if (candidateTokenized[j] in questionsTokenized[i]):\n",
    "                match = match + 1\n",
    "        similarity = match / (candidateNorm * questionNorm)\n",
    "        cosineSimilarity[i] = similarity\n",
    "\n",
    "    index_max = np.argmax(cosineSimilarity)\n",
    "    return(index_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER A GDP PERCENTAGE QUESTION\n",
    "# Accepts question as input and outputs the percentage effect on GDP\n",
    "\n",
    "def gdpPercent(question):\n",
    "    questionToken = nltk.word_tokenize(question)\n",
    "    if \"?\" in questionToken:\n",
    "        questionToken.remove(\"?\")\n",
    "    questionPOS = nltk.pos_tag(questionToken)\n",
    "\n",
    "    target = []\n",
    "    for i in range(len(questionToken)-1, -1, -1):\n",
    "        if questionPOS[i][1] != 'TO' and questionPOS[i][1] != 'IN':\n",
    "            target.insert(0, questionToken[i])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    query = ['GDP', 'affect', 'effect', 'effects','affects', 'decrease', 'increase', 'drag', '%', 'percent', 'growth']\n",
    "    for i in range(0,len(target)):\n",
    "        query.append(target[i])\n",
    "    gdpSearch = client.search(index=\"articles-index\", q=query, size=1000)\n",
    "    gdpHits = gdpSearch['hits']['hits']\n",
    "\n",
    "    candSentences = []\n",
    "    candTokenized = []\n",
    "    candScore = []\n",
    "    check = query\n",
    "    check.remove('GDP')\n",
    "    for i in np.arange(len(gdpHits)):\n",
    "        sentence = gdpHits[i]['_source']['body']\n",
    "        sentenceToken = nltk.word_tokenize(sentence)\n",
    "        score = 0\n",
    "        if 'GDP' in sentenceToken:\n",
    "            score = score + 2\n",
    "        for i in np.arange(len(target)):\n",
    "            if target[i] in sentenceToken:\n",
    "                score = score + 2\n",
    "        for i in np.arange(len(check)):\n",
    "            if check[i] in sentenceToken:\n",
    "                score = score + 1\n",
    "        candSentences.append(sentence)\n",
    "        candTokenized.append(sentenceToken)\n",
    "        candScore.append(score)\n",
    "\n",
    "    candDF = pd.DataFrame(data=[candSentences,candTokenized,candScore], index=['Sent','Token','Score'])\n",
    "    candDF = candDF.transpose()\n",
    "    candDF = candDF.sort(['Score'], ascending=False)\n",
    "    candDF.index = range(0,len(candDF['Sent']))\n",
    "\n",
    "    percentages = ['%','percent','percentage']\n",
    "    topsent = []\n",
    "    topscores = []\n",
    "    topanswers = []\n",
    "    for i in range(0,3):\n",
    "        topsent.append(candDF['Token'][i])\n",
    "        score = np.zeros(len(topsent[i]))\n",
    "        for j in range(0,len(score)):\n",
    "            gdpLeft = 100\n",
    "            gdpRight = 100\n",
    "            targLeft = 100\n",
    "            targRight = 100\n",
    "            ispercent = 0\n",
    "            if topsent[i][j] in percentages:\n",
    "                ispercent = 1\n",
    "            count = 0\n",
    "            for k in range(j, -1, -1):\n",
    "                if topsent[i][k] == \"GDP\":\n",
    "                    gdpLeft = count\n",
    "                    break\n",
    "                else:\n",
    "                    count = count + 1\n",
    "            count = 0\n",
    "            for k in range(j, -1, -1):\n",
    "                if topsent[i][k] == target[0]:\n",
    "                    targLeft = count\n",
    "                    break\n",
    "                else:\n",
    "                    count = count + 1           \n",
    "            count = 0\n",
    "            for k in range(j, len(topsent[i]), 1):\n",
    "                if topsent[i][k] == \"GDP\":\n",
    "                    gdpRight = count\n",
    "                    break\n",
    "                else:\n",
    "                    count = count + 1\n",
    "            count = 0\n",
    "            for k in range(j, len(topsent[i]), 1):\n",
    "                if topsent[i][k] == target[0]:\n",
    "                    targRight = count\n",
    "                    break\n",
    "                else:\n",
    "                    count = count + 1\n",
    "            gdpDist = min([gdpLeft, gdpRight])\n",
    "            targDist = min([targLeft, targRight])\n",
    "            score[j] = ispercent * 500 - gdpDist - targDist\n",
    "        topscores.append(score)\n",
    "        index, value = max(enumerate(topscores[i]), key=operator.itemgetter(1))\n",
    "        answer = [topsent[i][index-1], topsent[i][index]]\n",
    "        topanswers.append(answer)\n",
    "\n",
    "    for i in range(0,3):\n",
    "        if topanswers[i][1] in percentages:\n",
    "            topanswers[i][1] = \"percent\"\n",
    "            output = topanswers[0][0] + \" \"\n",
    "            output = output + topanswers[0][1]\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER A BANKRUPTCY QUESTION\n",
    "# Accepts question as input and returns the name of the company that went bankrupt\n",
    "\n",
    "def companyBankrupt(question):\n",
    "    questionToken = nltk.word_tokenize(question)\n",
    "    questionToken = [word for word in questionToken if not word in stop]\n",
    "    questionToken = nltk.pos_tag(questionToken)\n",
    "    query = ['bankrupt', 'bankruptcy', 'chapter 7', 'chapter 11', 'chapter 13', 'filed', 'declared']\n",
    "    date = []\n",
    "    for i in range(0,len(questionToken)):\n",
    "        pos = questionToken[i][1]\n",
    "        if (pos == 'NNP') or (pos == 'CD'):\n",
    "            query.append(questionToken[i][0])\n",
    "            date.append(questionToken[i][0])\n",
    "\n",
    "    query.append\n",
    "    bankruptSearch = client.search(index=\"articles-index\", q=query, size=5000)\n",
    "    candSentences = []\n",
    "    candTokenized = []\n",
    "    searchHits = bankruptSearch['hits']['hits']\n",
    "    for i in np.arange(len(searchHits)):\n",
    "        currSentence = searchHits[i]['_source']['body']\n",
    "        currTokenized = nltk.word_tokenize(currSentence)\n",
    "        currTokenized = [word for word in currTokenized if not word in stop]\n",
    "\n",
    "        includeDate = 1\n",
    "        includeBank = 0\n",
    "        Bank = ['bankruptcy', 'bankrupt', 'Chapter']\n",
    "        for i in range(0,len(date)):\n",
    "            if date[i] not in currTokenized:\n",
    "                includeDate = 0\n",
    "        for i in range(0, len(Bank)):\n",
    "            if Bank[i] in currTokenized:\n",
    "                includeBank = 1\n",
    "        if (includeDate == 1) and (includeBank == 1):\n",
    "            candSentences.append(currSentence)\n",
    "            candTokenized.append(currTokenized)\n",
    "\n",
    "    candidates = []\n",
    "    for i in np.arange(len(candSentences)):\n",
    "        candEntities = extract_entities(candSentences[i])\n",
    "        for j in np.arange(len(candEntities)):\n",
    "            candidates.append(candEntities[j])\n",
    "    \n",
    "    if candidates != []:\n",
    "        return max(set(candidates), key=candidates.count)\n",
    "    else:\n",
    "        print(\"Sorry, I could not find what you were looking for.\")\n",
    "        return -1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER A CEO NAME QUESTION\n",
    "# Accepts question as input and returns the name of that company's CEO\n",
    "\n",
    "def findCEO(question):\n",
    "    companyQuery = extract_entities(question)\n",
    "    if companyQuery == []:\n",
    "        print(\"Sorry, I could not find what you were looking for.\")\n",
    "        return -1\n",
    "    CEOsearch = client.search(index=\"articles-index\", q=companyQuery, size=100)\n",
    "    candSentences = []\n",
    "    for i in np.arange(len(CEOsearch['hits']['hits'])):\n",
    "        candSentences.append(CEOsearch['hits']['hits'][i]['_source']['body'])\n",
    "        candSentences[i] = nltk.word_tokenize(candSentences[i])\n",
    "    tokenQuestion = nltk.word_tokenize(companyQuery[0])\n",
    "    if \"of\" in tokenQuestion:\n",
    "        tokenQuestion.remove('of')\n",
    "    if len(tokenQuestion) == 1:\n",
    "        companyName = tokenQuestion[0]\n",
    "    else:\n",
    "        companyName = tokenQuestion[1]\n",
    "    if len(companyName) > 2:\n",
    "        for i in range(2, len(tokenQuestion)):\n",
    "            companyName = companyName + \" \"\n",
    "            companyName = companyName + tokenQuestion[i]\n",
    "        \n",
    "    reducedSentences = []\n",
    "    for i in np.arange(len(candSentences)):\n",
    "        keep = 1\n",
    "        for j in np.arange(len(tokenQuestion)):\n",
    "            if tokenQuestion[j] not in candSentences[i]:\n",
    "                keep = 0\n",
    "        if keep == 1:\n",
    "            reducedSentences.append(candSentences[i])\n",
    "            \n",
    "    candNames = []\n",
    "    for i in np.arange(len(reducedSentences)):\n",
    "        candEntities = extract_entities(reducedSentences[i])\n",
    "        for j in np.arange(len(candEntities)):\n",
    "            if (candEntities[j] != companyName) and (len(nltk.word_tokenize(candEntities[j]))==2):\n",
    "                candNames.append(candEntities[j])\n",
    "    \n",
    "    if candNames != []:\n",
    "        return max(set(candNames), key=candNames.count)\n",
    "    else:\n",
    "        print(\"Sorry, I could not find what you were looking for.\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANSWER QUESTION\n",
    "# Function accepts a sentence as input, runs the question classifier, and\n",
    "# selects the appropriate answering algorith to run\n",
    "\n",
    "def answerMyQuestion(candidate) :\n",
    "    questionType = classifyQuestion(candidate)\n",
    "    if questionType == 0 :\n",
    "        return(companyBankrupt(candidate))\n",
    "    elif questionType == 1 :\n",
    "        return(\"Consumption, consumer spending, government spending, investment, imports, exports, foreign trade\")\n",
    "    elif questionType == 2 :\n",
    "        return(gdpPercent(candidate))\n",
    "    elif questionType == 3 :\n",
    "        return(findCEO(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lehman Brothers'"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Which company went bankrupt in September 2008?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detroit'"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Which companies went bankrupt in July 2013?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US Airways'"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Which companies filed for bankruptcy in November of 2011?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consumption, consumer spending, government spending, investment, imports, exports, foreign trade'"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"What factors most affect GDP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5 percent'"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"What percentage drop or increase is associated with exports?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7 percent'"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"What percentage is associated with foreign trade?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 percent'"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"What change in GDP results from consumer spending?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Musk'"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Who is the CEO of Tesla?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark Zuckerberg'"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Who is the CEO of Facebook?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rich Handler'"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerMyQuestion(\"Who is the CEO at ZS Associates?\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
